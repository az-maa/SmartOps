import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt

####################### 1. Charger les données #########################
train_df = pd.read_csv("ServerMachineDataset/train/machine-1-1.txt", header=None)
test_df = pd.read_csv("ServerMachineDataset/test/machine-1-1.txt", header=None)
labels = pd.read_csv("ServerMachineDataset/test_label/machine-1-1.txt", header=None)

scaler = StandardScaler()
X_train = scaler.fit_transform(train_df)
X_test = scaler.transform(test_df)
y_true = labels.values.ravel()

####################### 2. Isolation Forest #########################
iso = IsolationForest(contamination=0.05, random_state=42)
iso.fit(X_train)
y_pred_iso = iso.predict(X_test)
y_pred_iso = np.where(y_pred_iso == -1, 1, 0)

print("\nIsolation Forest:")
print("Precision:", precision_score(y_true, y_pred_iso))
print("Recall   :", recall_score(y_true, y_pred_iso))
print("F1-score :", f1_score(y_true, y_pred_iso))

####################### 3. One-Class SVM #########################
svm = OneClassSVM(kernel="rbf", nu=0.05, gamma="scale")
svm.fit(X_train)
y_pred_svm = svm.predict(X_test)
y_pred_svm = np.where(y_pred_svm == -1, 1, 0)

print("\nOne-Class SVM:")
print("Precision:", precision_score(y_true, y_pred_svm))
print("Recall   :", recall_score(y_true, y_pred_svm))
print("F1-score :", f1_score(y_true, y_pred_svm))

####################### 4. Autoencoder #########################
input_dim = X_train.shape[1]
input_layer = Input(shape=(input_dim,))
encoded = Dense(16, activation="relu")(input_layer)
encoded = Dense(8, activation="relu")(encoded)
decoded = Dense(16, activation="relu")(encoded)
output_layer = Dense(input_dim, activation="linear")(decoded)

autoencoder = Model(inputs=input_layer, outputs=output_layer)
autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss="mse")

autoencoder.fit(X_train, X_train,
                epochs=20,
                batch_size=32,
                shuffle=True,
                verbose=1)

# Reconstruction error
X_test_pred = autoencoder.predict(X_test)
errors = np.mean(np.square(X_test - X_test_pred), axis=1)

####################### 5. Optimisation du seuil Autoencoder #########################
percentiles = range(80, 100, 2)  # seuils de 80 à 98 par pas de 2
results = []

for p in percentiles:
    threshold = np.percentile(errors, p)
    y_pred = (errors > threshold).astype(int)
    
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    
    results.append((p, threshold, precision, recall, f1))

# Meilleur seuil basé sur F1
best = max(results, key=lambda x: x[4])
print("\nAutoencoder (optimisation seuil):")
print(f"Meilleur percentile {best[0]} | Threshold={best[1]:.6f} | "
      f"Precision={best[2]:.3f} | Recall={best[3]:.3f} | F1={best[4]:.3f}")

####################### 6. Visualisation Precision-Recall-F1 #########################
precisions = [r[2] for r in results]
recalls = [r[3] for r in results]
f1_scores = [r[4] for r in results]

plt.figure(figsize=(10,6))
plt.plot(percentiles, precisions, marker='o', label="Precision")
plt.plot(percentiles, recalls, marker='s', label="Recall")
plt.plot(percentiles, f1_scores, marker='^', label="F1-score")
plt.title("Impact du seuil sur Precision, Recall et F1 (Autoencoder)")
plt.xlabel("Percentile du seuil")
plt.ylabel("Score")
plt.legend()
plt.grid(True)
plt.show()
